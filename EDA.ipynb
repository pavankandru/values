{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "df = pd.read_csv('data/arguments-training.tsv',delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: ylabel='Frequency'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Conclusion'].str.len().plot(kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(df['Premise'].str.len(),100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca50331c8e34e508754b4f0c76fdfea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/474 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "240ad22e7ae64f64a9fb8eaef2b88885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/533M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "441e0377570648279adf96a6b5a40016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d249d506240b4316b0a23ab9c25f752a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74129799c8444b0ca8cd6033dff95a6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModel,AutoTokenizer\n",
    "\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = df[['Premise','Conclusion','Stance']].apply(lambda x: tokenizer(x[0]+' [SEP] '+x[1]+' [SEP] '+x[2])['input_ids'],axis=1)\n",
    "# np.percentile(pl,q=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [1, 1437, 2, 1437, 0, 1437, 0, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/scratch/values/EDA.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bada_node/scratch/values/EDA.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model(torch\u001b[39m.\u001b[39;49mLongTensor(tokenizer(\u001b[39m'\u001b[39;49m\u001b[39m [SEP] [PAD] [PAD]\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:946\u001b[0m, in \u001b[0;36mDebertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[39mif\u001b[39;00m token_type_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    944\u001b[0m     token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(input_shape, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m--> 946\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membeddings(\n\u001b[1;32m    947\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    948\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m    949\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    950\u001b[0m     mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    951\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    952\u001b[0m )\n\u001b[1;32m    954\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\n\u001b[1;32m    955\u001b[0m     embedding_output,\n\u001b[1;32m    956\u001b[0m     attention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    959\u001b[0m     return_dict\u001b[39m=\u001b[39mreturn_dict,\n\u001b[1;32m    960\u001b[0m )\n\u001b[1;32m    961\u001b[0m encoded_layers \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/transformers/models/deberta/modeling_deberta.py:746\u001b[0m, in \u001b[0;36mDebertaEmbeddings.forward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, mask, inputs_embeds)\u001b[0m\n\u001b[1;32m    743\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    744\u001b[0m     input_shape \u001b[39m=\u001b[39m inputs_embeds\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> 746\u001b[0m seq_length \u001b[39m=\u001b[39m input_shape[\u001b[39m1\u001b[39;49m]\n\u001b[1;32m    748\u001b[0m \u001b[39mif\u001b[39;00m position_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    749\u001b[0m     position_ids \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_ids[:, :seq_length]\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.LongTensor(tokenizer([' [SEP] [PAD] [PAD]',])['input_ids'])\n",
    "print(a.shape)\n",
    "a = a.repeat([2,1,])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "classifier = nn.Sequential(*[nn.Linear(768,128),\n",
    "                                         nn.Linear(128,20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = model(a)['last_hidden_state'][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 20])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(emb).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import F1Score,Precision,Recall\n",
    "f1 = F1Score(task=\"multilabel\",num_labels=20)\n",
    "p = Precision(task='multilabel',num_labels=20)\n",
    "r = Recall(task='multilabel',num_labels=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaModel: ['lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight']\n",
      "- This IS expected if you are initializing DebertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from transformers import AutoModel,AutoTokenizer\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "model = AutoModel.from_pretrained(\"microsoft/deberta-base\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-base\")\n",
    "\n",
    "train_loader = DataLoader(Data(tokenizer,\"training\"),batch_size=16,collate_fn=collate_fn,shuffle=True,)\n",
    "val_loader = DataLoader(Data(tokenizer,\"validation\"),batch_size=16,collate_fn=collate_fn,)\n",
    "\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"values\",mode=\"disabled\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LightningModule\n",
    "class MLClassifier(pl.LightningModule):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model=model\n",
    "        self.classifier = nn.Sequential(*[nn.Linear(768,128),\n",
    "                                         nn.Linear(128,20)])\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self,x):\n",
    "        emb = self.model(x)['last_hidden_state'][:,0,:]\n",
    "        o = self.classifier(emb)\n",
    "        return o\n",
    "    \n",
    "    def prediction_reducer(self,otps):\n",
    "        predictions = torch.cat([i['predictions'].detach() for i in otps])\n",
    "        if 'labels' in otps[0]:\n",
    "            labels = torch.cat([i['labels'].detach() for i in otps])\n",
    "            return predictions,labels\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        X,y = batch\n",
    "        o = self.forward(X)\n",
    "        loss = self.loss_fn(o, y)\n",
    "        self.log(\"train_loss\", loss,on_step=True,on_epoch=True)\n",
    "        return {\"loss\": loss, \"predictions\": o,\"labels\":y}\n",
    "\n",
    "\n",
    "    def training_epoch_end(self, training_step_outputs):\n",
    "        predictions,labels = self.prediction_reducer(training_step_outputs)\n",
    "        print(predictions)\n",
    "        print(labels)\n",
    "        print(f1(predictions,labels))\n",
    "        # print(classification_report(labels.cpu(),(predictions>0.5).cpu()))\n",
    "\n",
    "    \n",
    "\n",
    "    def validation_step(self, batch,batch_idx):\n",
    "        # training_step defines the train loop.\n",
    "        # it is independent of forward\n",
    "        X,y = batch\n",
    "        o = self.forward(X)\n",
    "        loss = self.loss_fn(o, y)\n",
    "        self.log(\"Validation loss\", loss,on_epoch=True)\n",
    "        return {\"loss\": loss, \"predictions\": o,\"labels\":y}\n",
    "\n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        predictions,labels = self.prediction_reducer(validation_step_outputs)\n",
    "        # print(classification_report(labels.cpu(),(predictions>0.5).cpu()))\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # take average of `self.mc_iteration` iterations\n",
    "        X,y = batch\n",
    "        pred = self(X)\n",
    "        return pred,y\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# init the autoencoder\n",
    "clf = MLClassifier(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | model      | DebertaModel      | 138 M \n",
      "1 | classifier | Sequential        | 101 K \n",
      "2 | loss_fn    | BCEWithLogitsLoss | 0     \n",
      "-------------------------------------------------\n",
      "138 M     Trainable params\n",
      "0         Non-trainable params\n",
      "138 M     Total params\n",
      "554.811   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "147a75f0dc7f4ea0bfe29d003c2d6fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aec4db981ea4c9abd1d4da6f1ec7870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a581d9e5084357bd66607aa065a098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/torchmetrics/metric.py\", line 395, in wrapped_func\n",
      "    f\" `metric={self.__class__.__name__}(...).to(device)` where\"\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/torchmetrics/classification/stat_scores.py\", line 475, in update\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/torchmetrics/classification/stat_scores.py\", line 74, in _update_state\n",
      "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3398, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_6672/744120309.py\", line 2, in <cell line: 2>\n",
      "    trainer.fit(model=clf, train_dataloaders=train_loader,val_dataloaders = val_loader)\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 603, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/call.py\", line 38, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 645, in _fit_impl\n",
      "    self._run(model, ckpt_path=self.ckpt_path)\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1098, in _run\n",
      "    results = self._run_stage()\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1177, in _run_stage\n",
      "    self._run_train()\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1200, in _run_train\n",
      "    self.fit_loop.run()\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py\", line 200, in run\n",
      "    self.on_advance_end()\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py\", line 283, in on_advance_end\n",
      "    epoch_end_outputs = self.trainer._call_lightning_module_hook(\"training_epoch_end\", epoch_end_outputs)\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py\", line 1342, in _call_lightning_module_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "  File \"/tmp/ipykernel_6672/3502041046.py\", line 35, in training_epoch_end\n",
      "    print(f1(predictions,labels))\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1130, in _call_impl\n",
      "    return forward_call(*input, **kwargs)\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/torchmetrics/metric.py\", line 245, in forward\n",
      "    self.update(*args, **kwargs)\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/torchmetrics/metric.py\", line 309, in _forward_reduce_state_update\n",
      "    self._is_synced = False\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/torchmetrics/metric.py\", line 398, in wrapped_func\n",
      "    raise err\n",
      "RuntimeError: Encountered different devices in metric calculation (see stacktrace for details). This could be due to the metric class not being on the same device as input. Instead of `metric=MultilabelF1Score(...)` try to do `metric=MultilabelF1Score(...).to(device)` where device corresponds to the device of the input.\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1993, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 884, in structured_traceback\n",
      "    formatted_exceptions += self.format_exception_as_a_whole(etype, evalue, etb, lines_of_context,\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer( max_epochs=1,accelerator=\"gpu\", devices=1,logger=wandb_logger)\n",
    "trainer.fit(model=clf, train_dataloaders=train_loader,val_dataloaders = val_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:488: PossibleUserWarning: Your `predict_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "/home2/siri.venkata/miniconda3/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, predict_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "037102f79db54241adb8d3b8f50fad6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 338it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predictions = trainer.predict(model=clf,dataloaders=train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 20])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([torch.rand(16,20),torch.rand(8,20)]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "train_loader = DataLoader(Data(tokenizer,\"training\"),batch_size=16,collate_fn=collate_fn,shuffle=True,)\n",
    "val_loader = DataLoader(Data(tokenizer,\"validation\"),batch_size=16,collate_fn=collate_fn,)\n",
    "test_loader = DataLoader(Data(tokenizer,\"test\"),batch_size=16,collate_fn=collate_fn,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 06:56:58) \n[GCC 7.5.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8520a93c392975b9ccfdb99ad87895712a7403ecc90a6b822aa922151bf6b3f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
